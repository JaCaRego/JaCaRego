{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsUcddq99v138gLnd2/D1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaCaRego/JaCaRego/blob/main/Normalizacao_Padroniza%C3%A7%C3%A3o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c85b24d7"
      },
      "source": [
        "## NORMALIZAÇÃO E PADRONIZAÇÃO\n",
        "\n",
        "Este código realiza o *escalonamento de dados* (data scaling) em um DataFrame do pandas, aplicando três métodos diferentes às colunas 'idade' e 'salário':\n",
        "\n",
        "1.  **Normalização (MinMaxScaler)**: Transforma os dados para uma escala entre 0 e 1, ou -1 e 1. Isso garante que todos os valores das colunas sejam proporcionais dentro desse intervalo, sendo útil para algoritmos que dependem de distâncias ou são sensíveis à magnitude dos valores.\n",
        "2.  **Padronização (StandardScaler)**: Transforma os dados para que tenham uma média de 0 e um desvio padrão de 1. É útil quando a distribuição dos dados se aproxima de uma distribuição normal e para algoritmos que assumem dados centralizados.\n",
        "3.  **RobustScaler**: Similar ao StandardScaler, mas é mais robusto a *outliers* (valores atípicos) por usar a mediana e o Intervalo Interquartil (IQR) para escalar os dados.\n",
        "\n",
        "### Função Principal:\n",
        "\n",
        "*   **Preparação de Dados**: Carrega dados de um arquivo CSV (`clientes-v2-tratados.csv`).\n",
        "*   **Seleção de Features**: Foca nas colunas 'idade' e 'salário'.\n",
        "*   **Aplicação de Escalonamento**: Cria novas colunas para cada método de escalonamento aplicado.\n",
        "*   **Análise Pós-Escalonamento**: Imprime estatísticas (mínimo, máximo, média, desvio padrão) para cada coluna escalonada, permitindo comparar o efeito de cada técnica. A normalização é fundamental para tornar variáveis com escalas muito diferentes comparáveis e evitar que uma domine a análise ou o modelo de machine learning, como no exemplo de um sistema de crédito."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5d84c9"
      },
      "source": [
        "## Detalhamento das Fórmulas de Escalonamento e Definição de Termos\n",
        "\n",
        "Cada um dos escaladores possui uma fórmula matemática específica para transformar os dados, garantindo que o escalonamento seja aplicado de forma consistente. Além disso, incluímos as definições dos termos técnicos utilizados.\n",
        "\n",
        "### 1. MinMaxScaler (Normalização)\n",
        "\n",
        "O MinMaxScaler transforma as *features* (características ou atributos, as colunas ou variáveis independentes em um conjunto de dados) escalando cada *feature* individualmente para um determinado intervalo, geralmente entre 0 e 1, ou -1 e 1. Ele faz isso removendo o mínimo e dividindo pelo máximo menos o mínimo.\n",
        "\n",
        "**Fórmula:**\n",
        "$$ X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}} $$\n",
        "Onde:\n",
        "*   $X$ é o valor original do dado.\n",
        "*   $X_{min}$ é o valor mínimo da *feature*.\n",
        "*   $X_{max}$ é o valor máximo da *feature*.\n",
        "*   $X_{norm}$ é o valor normalizado.\n",
        "\n",
        "**Explicação:**\n",
        "Esta normalização é útil para algoritmos que não assumem uma distribuição específica para os dados, como redes neurais e algoritmos baseados em distância (**KNN** - *k-Nearest Neighbors*, um algoritmo que classifica um ponto de dados com base na maioria das classes de seus 'k' vizinhos mais próximos, e **SVM** - *Support Vector Machine*, um algoritmo que busca um hiperplano ótimo para separar classes), onde a escala dos dados pode influenciar diretamente a computação das distâncias. Ela comprime todos os dados para dentro de um intervalo fixo, mantendo a forma original da distribuição dos dados.\n",
        "\n",
        "### 2. StandardScaler (Padronização)\n",
        "\n",
        "O StandardScaler padroniza as *features* (características ou atributos) removendo a média e escalando para a variância da unidade. O resultado é que a distribuição dos dados terá uma média igual a 0 e um desvio padrão igual a 1. Isso é útil para *features* que seguem uma distribuição normal (gaussiana) ou que precisam ser centralizadas.\n",
        "\n",
        "**Fórmula:**\n",
        "$$ X_{pad} = \\frac{X - \\mu}{\\sigma} $$\n",
        "Onde:\n",
        "*   $X$ é o valor original do dado.\n",
        "*   $\\mu$ é a média da *feature*.\n",
        "*   $\\sigma$ é o desvio padrão da *feature*.\n",
        "*   $X_{pad}$ é o valor padronizado.\n",
        "\n",
        "**Explicação:**\n",
        "Essa padronização é particularmente importante para algoritmos que assumem que os dados são centralizados em 0 e têm uma variância semelhante, como **Regressão Linear** (um algoritmo de aprendizado supervisionado que modela a relação linear entre variáveis), **Regressão Logística** (um algoritmo de classificação usado para prever a probabilidade de um evento), **SVMs** (Máquinas de Vetores de Suporte, um algoritmo que busca um hiperplano ótimo para separar classes) com certos *kernels* (funções que transformam dados de baixa dimensão em um espaço de alta dimensão, facilitando a separação ou detecção de padrões não-lineares) e Redes Neurais. Ela lida bem com *outliers*, pois o cálculo da média e do desvio padrão é sensível a eles, mas o objetivo é trazer a distribuição para um formato padrão.\n",
        "\n",
        "### 3. RobustScaler\n",
        "\n",
        "O RobustScaler escala os *features* (características ou atributos) usando estatísticas que são robustas a *outliers* (valores atípicos ou discrepantes, que se desviam significativamente de outras observações). Ele remove a mediana e escala os dados usando o intervalo interquartil (IQR), que é a diferença entre o terceiro quartil (Q3) e o primeiro quartil (Q1).\n",
        "\n",
        "**Fórmula:**\n",
        "$$ X_{rob} = \\frac{X - Mediana}{IQR} $$\n",
        "Onde:\n",
        "*   $X$ é o valor original do dado.\n",
        "*   $Mediana$ é a mediana da *feature*.\n",
        "*   $IQR$ é o intervalo interquartil da *feature* ($Q_3 - Q_1$).\n",
        "*   $X_{rob}$ é o valor escalonado pelo RobustScaler.\n",
        "\n",
        "**Explicação:**\n",
        "Ao contrário do StandardScaler, o RobustScaler utiliza a mediana e o IQR, que são estatísticas menos afetadas por valores extremos (*outliers*). Isso o torna a escolha ideal quando o seu conjunto de dados contém muitos *outliers* e você não quer que eles influenciem a transformação de forma significativa. É especialmente útil em cenários onde a distribuição dos dados é assimétrica ou possui uma cauda longa devido a *outliers*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC8lZRcS7nLE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
        "\n",
        "\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df = pd.read_csv('dados/clientes-v2-tratados.csv')\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "#Pode usar qualquer um do dois\n",
        "#df = df.drop(['data', 'estado', 'nivel_educacao', 'numero_filhos', 'estado_civil', 'area_atuacao'], axis=1)\n",
        "df = df[['idade', 'salario']]\n",
        "\n",
        "# Normalização - MinMax/scaler.\n",
        "scaler = MinMaxScaler()\n",
        "df['idadeMinMaxScaler'] = scaler.fit_transform(df[['idade']])\n",
        "df['salarioMinMaxScaler'] = scaler.fit_transform(df[['salario']])\n",
        "\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "df['idadeMinMaxScaler_mm'] = min_max_scaler.fit_transform(df[['idade']])\n",
        "df['salarioMinMaxScaler_mm'] = min_max_scaler.fit_transform(df[['salario']])\n",
        "\n",
        "# Padronização - StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df['idadeStandardScaler'] = scaler.fit_transform(df[['idade']])\n",
        "df['salarioStandardScaler'] = scaler.fit_transform(df[['salario']])\n",
        "\n",
        "# Padronização RobustScaler\n",
        "scaler = RobustScaler()\n",
        "df['idadeRobustScaler'] = scaler.fit_transform(df[['idade']])\n",
        "df['salarioRobustScaler'] = scaler.fit_transform(df[['salario']])\n",
        "\n",
        "print(df.head(15))\n",
        "\n",
        "print('MinMaxScaler (De 0 a 1):')\n",
        "print('Idade - Min: {:.4f} Max: {:.4f} Mean: {:.4f} Std: {:.4f}'.format(df['idadeMinMaxScaler'].min(), df['idadeMinMaxScaler'].max(), df['idadeMinMaxScaler'].mean(), df['idadeMinMaxScaler'].std()))\n",
        "print('Salário - Min: {:.4f} Max: {:.4f} Mean: {:.4f} Std: {:.4f}'.format(df['salarioMinMaxScaler'].min(), df['salarioMinMaxScaler'].max(), df['salarioMinMaxScaler'].mean(), df['salarioMinMaxScaler'].std()))\n",
        "\n",
        "print('\\nMinMaxScaler (De -1 a 1):')\n",
        "print('Idade - Min: {:.4f} Max: {:.4f} Mean: {:.4f} Std: {:.4f}'.format(df['idadeMinMaxScaler_mm'].min(), df['idadeMinMaxScaler_mm'].max(), df['idadeMinMaxScaler'].mean(), df['idadeMinMaxScaler'].std()))\n",
        "print('Salário - Min: {:.4f} Max: {:.4f} Mean: {:.4f} Std: {:.4f}'.format(df['salarioMinMaxScaler_mm'].min(), df['salarioMinMaxScaler_mm'].max(), df['salarioMinMaxScaler_mm'].mean(), df['salarioMinMaxScaler_mm'].std()))\n",
        "\n",
        "print('\\nStandardScaler (Ajuste a média a 0 e desvio padrão a 1):')\n",
        "print('Idade - Min: {:.4f} Max: {:.4f} Mean: {:.18f} Std: {:.4f}'.format(df['idadeStandardScaler'].min(), df['idadeStandardScaler'].max(), df['idadeStandardScaler'].mean(), df['idadeStandardScaler'].std()))\n",
        "print('Salário - Min: {:.4f} Max: {:.4f} Mean: {:.18f} Std: {:.4f}'.format(df['salarioStandardScaler'].min(), df['salarioStandardScaler'].max(), df['salarioStandardScaler'].mean(), df['salarioStandardScaler'].std()))\n",
        "\n",
        "print('\\nRobustScaler (Ajuste a média e IQR):')\n",
        "print('Idade - Min: {:.4f} Max: {:.4f} Mean: {:.4f} Std: {:.4f}'.format(df['idadeRobustScaler'].min(), df['idadeRobustScaler'].max(), df['idadeRobustScaler'].mean(), df['idadeRobustScaler'].std()))\n",
        "print('Salário - Min: {:.4f} Max: {:.4f} Mean: {:.4f} Std: {:.4f}'.format(df['salarioRobustScaler'].min(), df['salarioRobustScaler'].max(), df['salarioRobustScaler'].mean(), df['salarioRobustScaler'].std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GBqZMS639FSl"
      }
    }
  ]
}